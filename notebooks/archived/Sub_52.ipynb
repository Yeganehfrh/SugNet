{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/yeganeh/Codes/SugNet/data/clean_data/sub-01_ses-01_task-baseline1_proc-clean_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     999.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "291 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr  # noqa\n",
    "from autoreject import AutoReject, get_rejection_threshold\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from mne_connectivity import envelope_correlation, spectral_connectivity_time\n",
    "\n",
    "from src.sugnet import amplitude_vector, dispersion_report, run_ica\n",
    "\n",
    "# constants\n",
    "# Network connectivit labels\n",
    "yeo7 = {\n",
    "    'N1': 'Visual',\n",
    "    'N2': 'Somatomotor',\n",
    "    'N3': 'DorsalAttention',\n",
    "    'N4': 'VentralAttention',\n",
    "    'N5': 'Limbic',\n",
    "    'N6': 'Frontoparietal',\n",
    "    'N7': 'Default',\n",
    "    'mwall': 'Medial_Wall',\n",
    "}\n",
    "\n",
    "hemisferes = ['lh', 'rh']\n",
    "\n",
    "# labels based on Yeo2011 atlas orders\n",
    "networks = [yeo7[k]+'_'+ hemisferes[i]\n",
    "            for k in yeo7.keys()\n",
    "            for i in range(len(hemisferes))]\n",
    "\n",
    "net_conn_labels = pd.DataFrame(index=networks, columns=networks)\n",
    "net_conn_labels = net_conn_labels.apply(lambda x: x.index + '\\N{left right arrow}' + x.name)\n",
    "net_conn_labels = net_conn_labels.values[np.tril_indices(net_conn_labels.shape[0], k=-1)]\n",
    "\n",
    "# sensor connectivity labels\n",
    "# merge subject data with all data\n",
    "# connectivity labels\n",
    "epochs_fname = 'data/clean_data/sub-01_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "epochs.drop_channels(ch_names=['M1', 'M2', 'EOG1', 'EOG2', 'ECG'])\n",
    "ch_names = epochs.ch_names.copy()\n",
    "sensor_conn_labels = pd.DataFrame(columns=ch_names, index=ch_names)\n",
    "sensor_conn_labels = sensor_conn_labels.apply(lambda x: x.index + ' \\N{left right arrow} ' + x.name)\n",
    "sensor_conn_labels = sensor_conn_labels.values[np.triu_indices(sensor_conn_labels.shape[0], k=1)]\n",
    "\n",
    "\n",
    "def preprocessing(raw,\n",
    "                  subject,\n",
    "                  task,\n",
    "                  ref_chs=None,\n",
    "                  filter_bounds=None,\n",
    "                  ica_n_components=None,\n",
    "                  autoreject=None,\n",
    "                  ica_report=True,\n",
    "                  n_job=-1):\n",
    "\n",
    "    # initialize empty containers for storing amplitude vectors, rejection threshold, etc.\n",
    "    pos = _make_montage()\n",
    "\n",
    "    # set channels positions\n",
    "    raw.set_montage(pos)\n",
    "\n",
    "    # interpolate bad channels\n",
    "    if raw.info['bads'] != []:\n",
    "        raw.interpolate_bads()\n",
    "\n",
    "    # filtering\n",
    "    if filter_bounds is not None:\n",
    "        raw.filter(\n",
    "            l_freq=filter_bounds[0],\n",
    "            h_freq=filter_bounds[1],\n",
    "            n_jobs=n_job\n",
    "            )\n",
    "\n",
    "    # create amplitude vector before ICA\n",
    "    ch_names = raw.ch_names\n",
    "\n",
    "    # apply ICA, remove eog and ecg ICs using templates, and save the report\n",
    "    raw = run_ica(raw, subject, task, n_components=ica_n_components, threshold=0.8, report=ica_report)\n",
    "\n",
    "    # epoching (note: for creating epochs with mne.epochs, tmin and tmax should be specified!)\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=1, preload=True)\n",
    "    del raw\n",
    "\n",
    "    # autoreject\n",
    "    if autoreject == 'global':\n",
    "        # pick only eeg channels for getting rejection threshold:\n",
    "        reject = get_rejection_threshold(epochs.copy().pick_types(eeg=True))\n",
    "        epochs.drop_bad(reject=reject)\n",
    "\n",
    "    if autoreject == 'local':\n",
    "        ar = AutoReject()  # TODO consider setting random state\n",
    "        epochs = ar.fit_transform(epochs)  # TODO check if this is ok to save the\n",
    "        # fit_transformed data to the save object\n",
    "\n",
    "    # rereferencing\n",
    "    if ref_chs is not None:\n",
    "        epochs.add_reference_channels(ref_channels='FCz')  # adding reference channel to the data\n",
    "        epochs.set_eeg_reference(ref_channels=ref_chs)\n",
    "    \n",
    "    \n",
    "    return epochs, reject\n",
    "\n",
    "\n",
    "def _make_montage(path='data/raw/plb-hyp-live2131111.vhdr'):\n",
    "    \"\"\"\n",
    "    Create a montage from barin vision raw data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to barinvision data header file\n",
    "\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_brainvision(path, verbose=False, misc=['ECG'])\n",
    "    raw.crop(1, 10)\n",
    "    raw.load_data().set_channel_types({'ECG': 'ecg'})\n",
    "    ch_names = copy.deepcopy(raw.info['ch_names'])\n",
    "    ch_names.remove('ECG')\n",
    "    pos_array = raw._get_channel_positions()\n",
    "\n",
    "    pos_dict = dict(zip(ch_names, pos_array))\n",
    "    pos = mne.channels.make_dig_montage(pos_dict)\n",
    "\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_session_data_for_sub(sessions_path, sub_id, task, session):\n",
    "    \"\"\"\n",
    "    Get the session data for a given subject\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sessions_path : str\n",
    "        path to the session data\n",
    "    sub_id : int\n",
    "        subject id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    session_data : pd.DataFrame\n",
    "        session data for a given subject\n",
    "\n",
    "    \"\"\"\n",
    "    condition = task + session\n",
    "    session_data = pd.read_excel(sessions_path)\n",
    "    session_data.set_index('bids_id', inplace=True)\n",
    "    session_data = session_data.loc[sub_id]\n",
    "    session_data = session_data[[f'hypnosis_depth_{session}', f'procedure_type_{session}',\n",
    "                                 f'description_type_{session}']]\n",
    "    session_data['session'] = session\n",
    "    session_data['condition'] = condition\n",
    "    session_data = session_data.to_frame().transpose()\n",
    "    session_data.reset_index(inplace=True)\n",
    "    session_data.index = [f'{sub_id}_{condition}']\n",
    "    session_data.columns = ['bids_id', 'hypnosis_depth', 'procedure', 'description', 'session', 'condition']\n",
    "\n",
    "    # reorder the columns\n",
    "    session_data = session_data[['bids_id', 'condition', 'hypnosis_depth', 'procedure', 'description', 'session']]\n",
    "\n",
    "    return session_data\n",
    "\n",
    "def merge_with_all(path, sub_data):\n",
    "    \"\"\"\n",
    "    Merge the data with the whole dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to the whole dataset\n",
    "    sub_data : pd.DataFrame\n",
    "        data for a single subject\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged : pd.DataFrame\n",
    "        merged data\n",
    "\n",
    "    \"\"\"\n",
    "    # open all data\n",
    "    all_data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    # update columns' types based on the whole dataset\n",
    "    types = [all_data[col].dtype for col in all_data.columns[:6]]\n",
    "\n",
    "    for col, t in zip(sub_data.columns[:6], types):\n",
    "        sub_data[col] = sub_data[col].astype(t)\n",
    "    \n",
    "    # update subdata index to be one more than the last index of the whole dataset\n",
    "    sub_data.index = [all_data.index[-1] + 1]\n",
    "\n",
    "    # concatenate the new data to the whole dataset\n",
    "    merged = pd.concat([all_data, sub_data])\n",
    "\n",
    "    # update columns' names (replace \"lowgamma\" with \"gamma\")\n",
    "    merged.columns = [col.replace('lowgamma', 'gamma') for col in merged.columns]\n",
    "\n",
    "    return merged\n",
    "\n",
    "def make_conn_df(conns, conn_labels, subject_id, task):\n",
    "    # initiate an empty df\n",
    "    conn_df = pd.DataFrame()\n",
    "    for freq, conn in conns.items():\n",
    "        conn_flat = conn[np.triu_indices(conn.shape[-1], k=1)]\n",
    "        freq_labels = conn_labels + f' ({freq})'\n",
    "        conn_df_ = pd.DataFrame(conn_flat, index=freq_labels, columns=[0]).transpose()\n",
    "        conn_df = conn_df.join(conn_df_, how='outer')\n",
    "        \n",
    "    conn_df = conn_df.set_axis([f'{subject_id}_{task}'])\n",
    "    \n",
    "    return conn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data and preprocess\n",
    "root = 'data/raw/sub-52/'\n",
    "subject = '52'\n",
    "task = 'experience2'\n",
    "\n",
    "# An example of a single subject\n",
    "subject_id = '214911relaxation.vhdr'\n",
    "subject_path = root + subject_id\n",
    "\n",
    "raw = mne.io.read_raw_brainvision(subject_path, preload=True, eog=('EOG1', 'EOG2'), misc=['ECG'])\n",
    "raw.set_channel_types({'ECG': 'ecg'})\n",
    "\n",
    "tmin = 2\n",
    "tmax = 302\n",
    "raw.crop(tmin=tmin, tmax=tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, rejects = preprocessing(raw, subject, task, 'average', [1, 42], 30, 'global')\n",
    "epochs.save(f'data/clean_data/sub-{subject}_ses-01_task-{task}_proc-clean_epo.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = 512 # we resample the data to 512 Hz\n",
    "frequencies = {'delta': (1, 4),\n",
    "               'theta': (4, 8),\n",
    "               'alpha': (8, 13),\n",
    "               'beta': (13, 30),\n",
    "               'lowgamma': (30, 42)  \n",
    "}\n",
    "# Helper functions\n",
    "def create_raw_from_epochs(epochs):\n",
    "    data = np.hstack(epochs.get_data())\n",
    "    info = mne.create_info(ch_names=epochs.ch_names,\n",
    "                           ch_types='eeg',\n",
    "                           sfreq=epochs.info['sfreq'])\n",
    "\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_channel_types({'ECG': 'ecg', 'EOG1': 'eog', 'EOG2': 'eog'})\n",
    "    return raw\n",
    "\n",
    "def make_forward():\n",
    "    # fsaverage files\n",
    "    fs_dir = Path('data/fsaverage')\n",
    "\n",
    "    # The files live in:\n",
    "    trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "    src = fs_dir / 'bem' / 'fsaverage-ico-4-src.fif' # use icosahedron4 with 6.2 mm source spacing\n",
    "    src = mne.read_source_spaces(src)\n",
    "    bem = fs_dir / 'bem' / 'fsaverage-5120-5120-5120-bem-sol.fif'\n",
    "    \n",
    "    path:Path = Path('data/clean_data')\n",
    "    epochs_fname = path / 'sub-01_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(sfreq)\n",
    "\n",
    "    # create raw object\n",
    "    raw = create_raw_from_epochs(epochs)\n",
    "\n",
    "    # insert channel positions\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    raw.set_montage(montage)\n",
    "\n",
    "    # forward solution (the same across all subjects)\n",
    "    fwd = mne.make_forward_solution(raw.info, trans=trans, src=src,\n",
    "                                    bem=bem, eeg=True, mindist=5.0, n_jobs=-1, verbose=False)\n",
    "    del src, bem\n",
    "    return fwd\n",
    "\n",
    "def make_inverse_4baseline(subject: str,\n",
    "                           fwd: mne.forward.forward.Forward,\n",
    "                           path = Path('data/clean_data')):\n",
    "    \n",
    "    epochs_fname = path / f'{subject}_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(512)\n",
    "    \n",
    "    # create raw object\n",
    "    raw = create_raw_from_epochs(epochs)\n",
    "\n",
    "    # insert channel positions\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    raw.set_eeg_reference('average', projection=True)\n",
    "    \n",
    "    # covariance matrix\n",
    "    cov = mne.compute_raw_covariance(raw, method='auto', cv=5, n_jobs=-1)\n",
    "\n",
    "    # inverse operator\n",
    "    inv = mne.minimum_norm.make_inverse_operator(raw.info, fwd, cov, verbose=False)\n",
    "    \n",
    "    return inv\n",
    "\n",
    "def get_labels(subject: str,\n",
    "                     task: str,\n",
    "                     atlas_labels: list,\n",
    "                     inv: mne.minimum_norm.inverse.InverseOperator,\n",
    "                     path:Path = Path('data/clean_data'),\n",
    "                     ):\n",
    "    # open data\n",
    "    epochs_fname = path / f'{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(512)\n",
    "    \n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    epochs.set_montage(montage)\n",
    "    \n",
    "    epochs.set_eeg_reference('average', projection=True)\n",
    "\n",
    "    stc = mne.minimum_norm.apply_inverse_epochs(epochs,\n",
    "                                                inv,\n",
    "                                                method='eLORETA',\n",
    "                                                lambda2=1./9.,\n",
    "                                                verbose=False)\n",
    "    \n",
    "    label_ts = mne.extract_label_time_course(stc,\n",
    "                                             atlas_labels,\n",
    "                                             inv['src'],\n",
    "                                             return_generator=False,\n",
    "                                             verbose=False)\n",
    "    \n",
    "    return label_ts\n",
    "\n",
    "def get_connectivity(label_ts,\n",
    "                     frequencies: dict,\n",
    "                     sfreq: int = sfreq\n",
    "):\n",
    "    def bp_gen(label_ts, fmin, fmax, sfreq):\n",
    "        for ts in label_ts:\n",
    "            yield mne.filter.filter_data(ts, sfreq=sfreq, l_freq=fmin, h_freq=fmax)\n",
    "    \n",
    "    # each segment in labels_ts is 1 second\n",
    "    # To compute the connectivity, we want segments that its lenght is about 30 seconds (or a bit less)\n",
    "    label_continious = np.hstack(np.array(label_ts))\n",
    "    label_ts = np.array_split(label_continious, 10, axis=1)\n",
    "    \n",
    "    conns = {} \n",
    "    for bp in frequencies.keys():\n",
    "        conn_obj = envelope_correlation(bp_gen(label_ts, frequencies[bp][0], frequencies[bp][1], sfreq),\n",
    "                                               orthogonalize='pairwise')\n",
    "        conn = conn_obj.combine()\n",
    "        conn = conn.get_data(output='dense')[..., 0]\n",
    "        conns[bp] = conn\n",
    "        \n",
    "    return conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "output_path_labels = f'data/parcellated_source_yeo7/{subject}_task-{task}_labels.npz'\n",
    "output_path_conn = f'data/connectivities/{subject}_task-{task}_conn-corr_filtered.pkl'\n",
    "\n",
    "# we will use Yeo2011 atlas (7 networks)\n",
    "atlas_labels = mne.read_labels_from_annot('fsaverage',\n",
    "                                          'Yeo2011_7Networks_N1000',\n",
    "                                          subjects_dir='data/')\n",
    "\n",
    "# forward solution (same across all subjects)\n",
    "fwd = make_forward()\n",
    "\n",
    "inv = make_inverse_4baseline('sub-52', fwd)\n",
    "label_ts = get_labels(subject, task, atlas_labels, inv)\n",
    "# np.savez_compressed(output_path_labels, labels=label_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEC\n",
    "#### at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claculate or read connectivity data\n",
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "output_path_conn = f'data/connectivities/{subject}_task-{task}_conn-corr_filtered.pkl'\n",
    "\n",
    "# first check if the connectivity data exists in the path\n",
    "if Path(output_path_conn).exists():\n",
    "    with open(output_path_conn, 'rb') as f:\n",
    "        conns = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    conns = get_connectivity(label_ts, frequencies)\n",
    "    with open(output_path_conn, 'wb') as f:\n",
    "            pickle.dump(conns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_df = make_conn_df(conns, net_conn_labels, subject, task)\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', 'sub-52', 'experience2')\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, conn_df], axis=1)\n",
    "sess_data_sub52.index = [258]\n",
    "\n",
    "merged = merge_with_all('data/classification_datasets/others/correlation_source.csv', sess_data_sub52)\n",
    "\n",
    "# save merged data\n",
    "# merged.to_csv('data/classification_datasets/correlation_source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "epochs_fname = f'data/clean_data/{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "\n",
    "frequencies = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'lowgamma': (30, 42)  \n",
    "}\n",
    "\n",
    "# helper function\n",
    "# Envelope Correlation at sensor level\n",
    "def get_PEC_sensor(epochs,\n",
    "                   frequencies: dict,\n",
    "                   sfreq: float = 1000):\n",
    "\n",
    "    def bp_gen(epochs, fmin, fmax, sfreq):\n",
    "        for epoch in epochs:\n",
    "            yield mne.filter.filter_data(epoch, sfreq=sfreq, l_freq=fmin, h_freq=fmax)\n",
    "    \n",
    "    # each segment in epochs is 1 second\n",
    "    raw = np.hstack(epochs.get_data())\n",
    "    \n",
    "    # To compute the connectivity, we want segments that its length is about 30 seconds (or a bit less)\n",
    "    epochs = np.array_split(raw, 10, axis=1)\n",
    "    \n",
    "    conns = {} \n",
    "    for bp in frequencies.keys():\n",
    "        conn_obj = envelope_correlation(bp_gen(epochs, frequencies[bp][0], frequencies[bp][1], sfreq),\n",
    "                                               orthogonalize='pairwise')\n",
    "        conn = conn_obj.combine()\n",
    "        conn = conn.get_data(output='dense')[..., 0]\n",
    "        conns[bp] = conn\n",
    "        \n",
    "    return conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate or read connectivity data\n",
    "connectivity_path = f'data/connectivities/correlation_sensor/{subject}_task-{task}_conn-corr_sensor_filtered.pkl'\n",
    "if Path(connectivity_path).exists():\n",
    "    with open(connectivity_path, 'rb') as f:\n",
    "        conns = pickle.load(f)\n",
    "else:\n",
    "    print(f'>>>>> Calculating Connectivity Data for {subject}, {task} task <<<<<')\n",
    "    # open epochs\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "\n",
    "    # set montage\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    epochs.set_montage(montage)\n",
    "        \n",
    "    # surface laplacian\n",
    "    epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs_csd.drop_channels(ch_names=['M1', 'M2', 'EOG1', 'EOG2', 'ECG'])\n",
    "\n",
    "    # compute connectivity\n",
    "    conns = get_PEC_sensor(epochs_csd, frequencies, sfreq=1000)\n",
    "    \n",
    "    # # save connectivity\n",
    "    # with open(connectivity_path, 'wb') as f:\n",
    "    #     pickle.dump(conns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_df = make_conn_df(conns, sensor_conn_labels, '52', task)\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, conn_df], axis=1)\n",
    "sess_data_sub52.index = [258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_with_all('data/classification_datasets/others/correlation_sensor.csv', sess_data_sub52)\n",
    "# save merged data\n",
    "merged.to_csv('data/classification_datasets/correlation_sensor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wPLI\n",
    "#### at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "freqs = np.arange(1, 42, 1)\n",
    "foi = np.array([\n",
    "        [1, 4],\n",
    "        [4, 8],\n",
    "        [8, 13],\n",
    "        [13, 30],\n",
    "        [30, 42]\n",
    "        ])\n",
    "fmin = tuple(foi[:, 0])\n",
    "fmax = tuple(foi[:, 1])\n",
    "\n",
    "## bands\n",
    "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "# define a function that get a numpy array and return a dataframe of FCs\n",
    "def calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, conn_labels, sfreq,\n",
    "                   verbose=0, decim=20):\n",
    "  conn = spectral_connectivity_time(data,\n",
    "                                    freqs=freqs,\n",
    "                                    method='wpli',\n",
    "                                    fmin=fmin,\n",
    "                                    fmax=fmax,\n",
    "                                    sfreq=sfreq,\n",
    "                                    faverage=True,\n",
    "                                    average=True,\n",
    "                                    verbose=verbose,\n",
    "                                    decim=decim,\n",
    "                                    n_cycles=5\n",
    "                                  )\n",
    "\n",
    "  # get data\n",
    "  conn = conn.get_data(output='dense')\n",
    "  \n",
    "  # create a dataframe from a dictionary of connectivities in different bands\n",
    "  temp = {}\n",
    "  for i, b in enumerate(bands):\n",
    "      temp[b] = conn[:, :, i][np.tril_indices(conn.shape[0], k=-1)]\n",
    "\n",
    "  df = pd.DataFrame(temp, index=conn_labels).stack().reset_index()\n",
    "  df['conn'] = df['level_0'] + '_' + df['level_1']\n",
    "  \n",
    "  return df.drop(columns=['level_0', 'level_1']).set_index('conn').T.set_index([pd.Index([subject + '_' + task])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "subject_path = f'data/parcellated_source_yeo7/{subject}_task-{task}_labels.npz'\n",
    "label_ts = np.load(subject_path)['labels']\n",
    "data = np.hstack(label_ts)\n",
    "\n",
    "# To get 30-seconds time segments, we need to split source arrays into 10 segments.\n",
    "# To make the array divisible by 10, we cut off a few data points (less than 10) at the end:\n",
    "idx = data.shape[1] - data.shape[1]%10\n",
    "data = data[:, :idx]\n",
    "data = np.array(np.split(data, 10, axis=1))\n",
    "\n",
    "# calculate wpli\n",
    "df = calculate_wpli(data, freqs, fmin, fmax, bands, subject=subject, task=task, conn_labels=net_conn_labels,\n",
    "                    verbose=0, decim=10, sfreq=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session data for a single subject\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "df.index = ['52_experience2'] # rename connectivity index to make it match with session data index\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, df], axis=1)\n",
    "# # merge subject data with all data\n",
    "merged = merge_with_all('data/classification_datasets/others/wpli_source.csv', sess_data_sub52)\n",
    "# merged.to_csv('data/classification_datasets/wpli_source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_fname = f'data/clean_data/{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "\n",
    "epochs = mne.read_epochs(epochs_fname, verbose=0)\n",
    "epochs.drop_channels(['M1', 'M2'])\n",
    "epochs.pick_types(eeg=True)\n",
    "\n",
    "# set montage\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "epochs.set_montage(montage)\n",
    "print(epochs.info)\n",
    "\n",
    "# surface laplacian\n",
    "epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "data = np.array(np.split(np.hstack(epochs_csd.get_data()), 10, axis=1))\n",
    "\n",
    "df = calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, sensor_conn_labels, sfreq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session data for a single subject\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "df.index = ['52_experience2'] # rename connectivity index to make it match with session data index\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, df], axis=1)\n",
    "# # merge subject data with all data\n",
    "merged = merge_with_all('data/classification_datasets/others/wpli_sensor.csv', sess_data_sub52)\n",
    "merged.to_csv('data/classification_datasets/wpli_sensor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power\n",
    "#### at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant variables to be used in the code\n",
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "epochs_fname = f'data/clean_data/{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "source_path = f'data/parcellated_source_yeo7/{subject}_task-{task}_labels.npz'\n",
    "\n",
    "# sampling rate\n",
    "sfreq = 512\n",
    "\n",
    "# frequency bands of interest\n",
    "frequencies = {'delta': (1, 3.875),\n",
    "               'theta': (4, 7.875),\n",
    "               'alpha': (8, 12.875),\n",
    "               'beta': (13, 30),\n",
    "               'lowgamma': (30.125, 42) \n",
    "}\n",
    "# bands = Bands(frequencies)\n",
    "\n",
    "#helper functions\n",
    "# get dataframe of powers for each frequency band\n",
    "def get_power_bands_df(psds, freqs, labels=networks):\n",
    "    \n",
    "    # initiate a dictionary to store power from each band\n",
    "    psds_bands = {}\n",
    "    \n",
    "    # average the power over each band\n",
    "    for k in frequencies.keys():\n",
    "        temp = psds[:, np.where((frequencies[k][0] <= freqs) & (freqs <= frequencies[k][1]) == True)[0]]\n",
    "        psds_bands[k] = temp.mean(1)\n",
    "    \n",
    "    # create and return a dataframe with the power from each band across all networks\n",
    "    df = pd.DataFrame.from_dict(psds_bands, orient='index', columns=labels).T.stack().reset_index()\n",
    "    df['new_col'] = df['level_0'] + '_' + df['level_1']\n",
    "    df.drop(['level_0', 'level_1'], axis=1, inplace=True)\n",
    "    \n",
    "    return df.set_index('new_col').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 8.000 (s)\n"
     ]
    }
   ],
   "source": [
    "label_ts = np.load(source_path)['labels']\n",
    "  \n",
    "# create a continuous data array from the parcellated source data\n",
    "label_continious = np.hstack(np.array(label_ts))\n",
    "\n",
    "# calculate psd on continuous data\n",
    "psds, freqs = psd_array_welch(label_continious,\n",
    "                              sfreq=sfreq,\n",
    "                              fmin=1,\n",
    "                              fmax=42,\n",
    "                              n_fft=4096, # window size is 4096/512 = 8s\n",
    "                              n_overlap=2048, # 50% overlap\n",
    "                              n_jobs=1)\n",
    "\n",
    "# extract periodic parameters\n",
    "df_periodic = get_power_bands_df(psds, freqs=freqs)\n",
    "\n",
    "# reindex the dataframe\n",
    "df_periodic.index = [subject[4:] + '_' + task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session data for a single subject\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, df_periodic], axis=1)\n",
    "# # merge subject data with all data\n",
    "merged = merge_with_all('data/classification_datasets/others/power_source.csv', sess_data_sub52)\n",
    "# merged.to_csv('data/classification_datasets/power_source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## channels name\n",
    "epochs = mne.read_epochs('data/clean_data/sub-01_ses-01_task-baseline1_proc-clean_epo.fif', verbose=0)\n",
    "ch_names = epochs.ch_names.copy()  # make sure to copy the list because it is mutable in place\n",
    "[ch_names.remove(i) for i in ['M1', 'M2', 'EOG1', 'EOG2', 'ECG']]\n",
    "all_channels = epochs.ch_names\n",
    "\n",
    "# name of electrode groups\n",
    "ba_patches = {'LF': ['Fp1', 'F3', 'F7', 'AF3', 'F1', 'F5'],\n",
    " 'LC': ['C3', 'T7', 'FC1', 'FC3', 'FC5', 'C1', 'C5', 'FT7'],\n",
    " 'LP': ['P3', 'P7', 'CP1', 'CP3', 'CP5', 'TP7', 'P1', 'P5'],\n",
    " 'LO': ['O1', 'PO3'],\n",
    " 'RF': ['Fp2', 'F4', 'F8', 'AF4', 'F2', 'F6',],\n",
    " 'RC': ['C4', 'T8', 'FC2', 'FC4', 'FC6', 'C2', 'C6', 'FT8'],\n",
    " 'RP': ['P4', 'P8', 'CP2', 'CP4', 'CP6', 'TP8', 'P2', 'P6'],\n",
    " 'RO': ['O2', 'PO4'],\n",
    " 'FZ': ['Fpz', 'Fz'],\n",
    " 'CZ': ['Cz', 'FCz'],\n",
    " 'PZ': ['Pz', 'CPz'],\n",
    " 'OZ': ['POz', 'Oz', 'Iz'],\n",
    " 'all': ch_names\n",
    "}\n",
    "\n",
    "# index of electrode groups\n",
    "ba_patches_ind = {}\n",
    "for k,v in ba_patches.items():\n",
    "    temp = [all_channels.index(i) for i in v]\n",
    "    ba_patches_ind[k] = temp\n",
    "\n",
    "# frequency indces\n",
    "freq = dict(delta=(0, 24),\n",
    "            theta=(24, 56),\n",
    "            alpha=(56, 96),\n",
    "            beta=(96, 233),\n",
    "            gamma=(233, 330))\n",
    "\n",
    "############ Helper Functions ############\n",
    "def calculate_psd(epochs_fname,\n",
    "                  save=False):\n",
    "    \n",
    "    psds_dict = {}\n",
    "    epochs = mne.read_epochs(epochs_fname)\n",
    "    data = np.hstack(epochs.get_data())\n",
    "    psds, _ = psd_array_welch(data,\n",
    "                              sfreq=1000,\n",
    "                              fmin=1,\n",
    "                              fmax=42,\n",
    "                              n_fft=8000,\n",
    "                              verbose=0\n",
    "                              )\n",
    "    psds_dict[subject+'_'+task] = psds\n",
    "    return psds_dict\n",
    "    \n",
    "def aggregate_psds(psds_dict, ba_patches_ind, freq):\n",
    "    \"\"\"Aggregate PSDs across channels and frequency bands.\"\"\"\n",
    "    # create a dataframe from aggreagated data, power in picovolts\n",
    "    psds_agg = {}\n",
    "    for k1, v1 in psds_dict.items():\n",
    "        for k2, v2 in ba_patches_ind.items():\n",
    "            for k3, v3 in freq.items():\n",
    "                psds_agg[k1+'-'+k2+'_'+k3] = v1[v2].mean(0)[v3[0]:v3[1]].mean(0) * 10000 ** 3 #picovolts\n",
    "    return psds_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/yeganeh/Codes/SugNet/data/clean_data/sub-52_ses-01_task-experience2_proc-clean_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     999.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "296 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/qy0qc5tx7t7_97xjmy6pvksc0000gn/T/ipykernel_72017/3289573305.py:42: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  data = np.hstack(epochs.get_data())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sub-52_experience2-LF_delta': 4.892521786715944,\n",
       " 'sub-52_experience2-LF_theta': 0.9543860275813599,\n",
       " 'sub-52_experience2-LF_alpha': 1.2209368922444404,\n",
       " 'sub-52_experience2-LF_beta': 0.4315413036384727,\n",
       " 'sub-52_experience2-LF_gamma': 0.22736528887596727,\n",
       " 'sub-52_experience2-LC_delta': 5.106679118335261,\n",
       " 'sub-52_experience2-LC_theta': 1.031542644728039,\n",
       " 'sub-52_experience2-LC_alpha': 1.0693203543227126,\n",
       " 'sub-52_experience2-LC_beta': 0.44457140159289227,\n",
       " 'sub-52_experience2-LC_gamma': 0.37459597368431974,\n",
       " 'sub-52_experience2-LP_delta': 1.8522353085230137,\n",
       " 'sub-52_experience2-LP_theta': 0.9009803332145465,\n",
       " 'sub-52_experience2-LP_alpha': 2.0042830132649114,\n",
       " 'sub-52_experience2-LP_beta': 0.6206558721276556,\n",
       " 'sub-52_experience2-LP_gamma': 0.17280548352256098,\n",
       " 'sub-52_experience2-LO_delta': 5.978224793763823,\n",
       " 'sub-52_experience2-LO_theta': 1.4818193222901468,\n",
       " 'sub-52_experience2-LO_alpha': 4.003589800633029,\n",
       " 'sub-52_experience2-LO_beta': 0.6441777701275091,\n",
       " 'sub-52_experience2-LO_gamma': 0.11721145474454918,\n",
       " 'sub-52_experience2-RF_delta': 10.108075006514202,\n",
       " 'sub-52_experience2-RF_theta': 1.766306057893496,\n",
       " 'sub-52_experience2-RF_alpha': 1.7036559894343068,\n",
       " 'sub-52_experience2-RF_beta': 1.0023060727935242,\n",
       " 'sub-52_experience2-RF_gamma': 0.9048060737594535,\n",
       " 'sub-52_experience2-RC_delta': 2.5375475569931867,\n",
       " 'sub-52_experience2-RC_theta': 0.8485539925506133,\n",
       " 'sub-52_experience2-RC_alpha': 1.1415821459068223,\n",
       " 'sub-52_experience2-RC_beta': 0.8806503350140761,\n",
       " 'sub-52_experience2-RC_gamma': 1.0461971081352033,\n",
       " 'sub-52_experience2-RP_delta': 1.6663410310144915,\n",
       " 'sub-52_experience2-RP_theta': 1.0062618638888194,\n",
       " 'sub-52_experience2-RP_alpha': 2.1307250032722505,\n",
       " 'sub-52_experience2-RP_beta': 0.566597654750221,\n",
       " 'sub-52_experience2-RP_gamma': 0.3361624763634807,\n",
       " 'sub-52_experience2-RO_delta': 3.0421590058094115,\n",
       " 'sub-52_experience2-RO_theta': 1.1479086579794926,\n",
       " 'sub-52_experience2-RO_alpha': 2.3643813352361085,\n",
       " 'sub-52_experience2-RO_beta': 0.45587779267784795,\n",
       " 'sub-52_experience2-RO_gamma': 0.18506880999307734,\n",
       " 'sub-52_experience2-FZ_delta': 2.511429492207603,\n",
       " 'sub-52_experience2-FZ_theta': 1.3765320476994396,\n",
       " 'sub-52_experience2-FZ_alpha': 1.6208426128909874,\n",
       " 'sub-52_experience2-FZ_beta': 0.4987799190634207,\n",
       " 'sub-52_experience2-FZ_gamma': 0.2072443282927925,\n",
       " 'sub-52_experience2-CZ_delta': 1.4034566912826456,\n",
       " 'sub-52_experience2-CZ_theta': 1.0326840736563105,\n",
       " 'sub-52_experience2-CZ_alpha': 1.061173789485482,\n",
       " 'sub-52_experience2-CZ_beta': 0.25392451449035175,\n",
       " 'sub-52_experience2-CZ_gamma': 0.05597545531953276,\n",
       " 'sub-52_experience2-PZ_delta': 1.0451707144824498,\n",
       " 'sub-52_experience2-PZ_theta': 1.1022187715884406,\n",
       " 'sub-52_experience2-PZ_alpha': 1.777701127584657,\n",
       " 'sub-52_experience2-PZ_beta': 0.7478716361358095,\n",
       " 'sub-52_experience2-PZ_gamma': 0.04057021087505001,\n",
       " 'sub-52_experience2-OZ_delta': 1.5171854305781116,\n",
       " 'sub-52_experience2-OZ_theta': 0.9925394939049417,\n",
       " 'sub-52_experience2-OZ_alpha': 2.498852767070126,\n",
       " 'sub-52_experience2-OZ_beta': 0.40880835408289634,\n",
       " 'sub-52_experience2-OZ_gamma': 0.09865308483881997,\n",
       " 'sub-52_experience2-all_delta': 3.7161130274585035,\n",
       " 'sub-52_experience2-all_theta': 1.085663288000838,\n",
       " 'sub-52_experience2-all_alpha': 1.7099434571417014,\n",
       " 'sub-52_experience2-all_beta': 0.6163258718194614,\n",
       " 'sub-52_experience2-all_gamma': 0.41647747863900886}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psds_dict = calculate_psd(epochs_fname)\n",
    "psds_agg = aggregate_psds(psds_dict, ba_patches_ind, freq)\n",
    "psds_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub-52_experience2': array([[8.92903172e-12, 7.84996205e-12, 6.58383492e-12, ...,\n",
       "         3.44756695e-13, 2.82732296e-13, 2.38741841e-13],\n",
       "        [4.98796117e-11, 4.04585345e-11, 4.15177696e-11, ...,\n",
       "         4.93802499e-13, 5.32063204e-13, 5.82474023e-13],\n",
       "        [4.07001780e-12, 3.70842352e-12, 4.57724059e-12, ...,\n",
       "         8.83971160e-14, 9.24446141e-14, 6.66867137e-14],\n",
       "        ...,\n",
       "        [9.92837041e-12, 8.00557987e-12, 5.58430869e-12, ...,\n",
       "         7.02463258e-14, 5.20172497e-14, 5.74307540e-14],\n",
       "        [1.42892348e-11, 1.57358464e-11, 1.15148502e-11, ...,\n",
       "         1.02368198e-13, 9.46011765e-14, 1.42115199e-13],\n",
       "        [3.07785375e-12, 2.82826174e-12, 2.05873624e-12, ...,\n",
       "         6.37379717e-14, 6.21788265e-14, 5.92624144e-14]])}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/PLB_HYP_data_MASTER.xlsx')\n",
    "id_vars=['timestamp_trigger_4_recording_1',\n",
    " 'timestamp_trigger_4_recording_2',\n",
    " 'timestamp_trigger_4_recording_3',\n",
    " 'timestamp_trigger_4_recording_4',\n",
    " 'timestamp_trigger_5_recording_1',\n",
    " 'timestamp_trigger_5_recording_2',\n",
    " 'timestamp_trigger_5_recording_3',\n",
    " 'timestamp_trigger_5_recording_4',\n",
    " 'timestamp_trigger_6_recording_1',\n",
    " 'timestamp_trigger_6_recording_2',\n",
    " 'timestamp_trigger_6_recording_3',\n",
    " 'timestamp_trigger_6_recording_4',\n",
    " 'timestamp_trigger_7_recording_1',\n",
    " 'timestamp_trigger_7_recording_2',\n",
    " 'timestamp_trigger_7_recording_3',\n",
    " 'timestamp_trigger_7_recording_4',\n",
    " ]\n",
    "\n",
    "df = df.melt(id_vars=id_vars,\n",
    " value_vars=['procedure_type_1',\n",
    " 'procedure_type_2',\n",
    " 'procedure_type_3',\n",
    " 'procedure_type_4'])\n",
    "\n",
    "df['session'] = df['variable'].apply(lambda x: x.split('_')[-1])\n",
    "df['timestamp'] = df.apply(lambda r: r['timestamp_trigger_4_recording_'+r['session']], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SugNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
