{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/yeganeh/Codes/SugNet/data/clean_data/sub-01_ses-01_task-baseline1_proc-clean_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     999.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "291 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr  # noqa\n",
    "from autoreject import AutoReject, get_rejection_threshold\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from mne_connectivity import envelope_correlation, spectral_connectivity_time\n",
    "\n",
    "from src.sugnet import amplitude_vector, dispersion_report, run_ica\n",
    "\n",
    "# constants\n",
    "# Network connectivit labels\n",
    "yeo7 = {\n",
    "    'N1': 'Visual',\n",
    "    'N2': 'Somatomotor',\n",
    "    'N3': 'DorsalAttention',\n",
    "    'N4': 'VentralAttention',\n",
    "    'N5': 'Limbic',\n",
    "    'N6': 'Frontoparietal',\n",
    "    'N7': 'Default',\n",
    "    'mwall': 'Medial_Wall',\n",
    "}\n",
    "\n",
    "hemisferes = ['lh', 'rh']\n",
    "\n",
    "# labels based on Yeo2011 atlas orders\n",
    "networks = [yeo7[k]+'_'+ hemisferes[i] for k in yeo7.keys() for i in range(len(hemisferes))]\n",
    "\n",
    "net_conn_labels = pd.DataFrame(index=networks, columns=networks)\n",
    "net_conn_labels = net_conn_labels.apply(lambda x: x.index + '\\N{left right arrow}' + x.name)\n",
    "net_conn_labels = net_conn_labels.values[np.tril_indices(net_conn_labels.shape[0], k=-1)]\n",
    "\n",
    "# sensor connectivity labels\n",
    "# merge subject data with all data\n",
    "# connectivity labels\n",
    "epochs_fname = 'data/clean_data/sub-01_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "epochs.drop_channels(ch_names=['M1', 'M2', 'EOG1', 'EOG2', 'ECG'])\n",
    "ch_names = epochs.ch_names.copy()\n",
    "sensor_conn_labels = pd.DataFrame(columns=ch_names, index=ch_names)\n",
    "sensor_conn_labels = sensor_conn_labels.apply(lambda x: x.index + ' \\N{left right arrow} ' + x.name)\n",
    "sensor_conn_labels = sensor_conn_labels.values[np.triu_indices(sensor_conn_labels.shape[0], k=1)]\n",
    "\n",
    "\n",
    "def preprocessing(raw,\n",
    "                  subject,\n",
    "                  task,\n",
    "                  ref_chs=None,\n",
    "                  filter_bounds=None,\n",
    "                  ica_n_components=None,\n",
    "                  autoreject=None,\n",
    "                  ica_report=True,\n",
    "                  n_job=-1):\n",
    "\n",
    "    # initialize empty containers for storing amplitude vectors, rejection threshold, etc.\n",
    "    pos = _make_montage()\n",
    "\n",
    "    # set channels positions\n",
    "    raw.set_montage(pos)\n",
    "\n",
    "    # interpolate bad channels\n",
    "    if raw.info['bads'] != []:\n",
    "        raw.interpolate_bads()\n",
    "\n",
    "    # filtering\n",
    "    if filter_bounds is not None:\n",
    "        raw.filter(\n",
    "            l_freq=filter_bounds[0],\n",
    "            h_freq=filter_bounds[1],\n",
    "            n_jobs=n_job\n",
    "            )\n",
    "\n",
    "    # create amplitude vector before ICA\n",
    "    ch_names = raw.ch_names\n",
    "\n",
    "    # apply ICA, remove eog and ecg ICs using templates, and save the report\n",
    "    raw = run_ica(raw, subject, task, n_components=ica_n_components, threshold=0.8, report=ica_report)\n",
    "\n",
    "    # epoching (note: for creating epochs with mne.epochs, tmin and tmax should be specified!)\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=1, preload=True)\n",
    "    del raw\n",
    "\n",
    "    # autoreject\n",
    "    if autoreject == 'global':\n",
    "        # pick only eeg channels for getting rejection threshold:\n",
    "        reject = get_rejection_threshold(epochs.copy().pick_types(eeg=True))\n",
    "        epochs.drop_bad(reject=reject)\n",
    "\n",
    "    if autoreject == 'local':\n",
    "        ar = AutoReject()  # TODO consider setting random state\n",
    "        epochs = ar.fit_transform(epochs)  # TODO check if this is ok to save the\n",
    "        # fit_transformed data to the save object\n",
    "\n",
    "    # rereferencing\n",
    "    if ref_chs is not None:\n",
    "        epochs.add_reference_channels(ref_channels='FCz')  # adding reference channel to the data\n",
    "        epochs.set_eeg_reference(ref_channels=ref_chs)\n",
    "    \n",
    "    \n",
    "    return epochs, reject\n",
    "\n",
    "\n",
    "def _make_montage(path='data/raw/plb-hyp-live2131111.vhdr'):\n",
    "    \"\"\"\n",
    "    Create a montage from barin vision raw data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to barinvision data header file\n",
    "\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_brainvision(path, verbose=False, misc=['ECG'])\n",
    "    raw.crop(1, 10)\n",
    "    raw.load_data().set_channel_types({'ECG': 'ecg'})\n",
    "    ch_names = copy.deepcopy(raw.info['ch_names'])\n",
    "    ch_names.remove('ECG')\n",
    "    pos_array = raw._get_channel_positions()\n",
    "\n",
    "    pos_dict = dict(zip(ch_names, pos_array))\n",
    "    pos = mne.channels.make_dig_montage(pos_dict)\n",
    "\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_session_data_for_sub(sessions_path, sub_id, task, session):\n",
    "    \"\"\"\n",
    "    Get the session data for a given subject\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sessions_path : str\n",
    "        path to the session data\n",
    "    sub_id : int\n",
    "        subject id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    session_data : pd.DataFrame\n",
    "        session data for a given subject\n",
    "\n",
    "    \"\"\"\n",
    "    condition = task + session\n",
    "    session_data = pd.read_excel(sessions_path)\n",
    "    session_data.set_index('bids_id', inplace=True)\n",
    "    session_data = session_data.loc[sub_id]\n",
    "    session_data = session_data[[f'hypnosis_depth_{session}', f'procedure_type_{session}',\n",
    "                                 f'description_type_{session}']]\n",
    "    session_data['session'] = session\n",
    "    session_data['condition'] = condition\n",
    "    session_data = session_data.to_frame().transpose()\n",
    "    session_data.reset_index(inplace=True)\n",
    "    session_data.index = [f'{sub_id}_{condition}']\n",
    "    session_data.columns = ['bids_id', 'hypnosis_depth', 'procedure', 'description', 'session', 'condition']\n",
    "\n",
    "    # reorder the columns\n",
    "    session_data = session_data[['bids_id', 'condition', 'hypnosis_depth', 'procedure', 'description', 'session']]\n",
    "\n",
    "    return session_data\n",
    "\n",
    "def merge_with_all(path, sub_data):\n",
    "    \"\"\"\n",
    "    Merge the data with the whole dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to the whole dataset\n",
    "    sub_data : pd.DataFrame\n",
    "        data for a single subject\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged : pd.DataFrame\n",
    "        merged data\n",
    "\n",
    "    \"\"\"\n",
    "    # open all data\n",
    "    all_data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    # update columns' types based on the whole dataset\n",
    "    types = [all_data[col].dtype for col in all_data.columns[:6]]\n",
    "\n",
    "    for col, t in zip(sub_data.columns[:6], types):\n",
    "        sub_data[col] = sub_data[col].astype(t)\n",
    "    \n",
    "    # update subdata index to be one more than the last index of the whole dataset\n",
    "    sub_data.index = [all_data.index[-1] + 1]\n",
    "\n",
    "    # concatenate the new data to the whole dataset\n",
    "    merged = pd.concat([all_data, sub_data])\n",
    "\n",
    "    # update columns' names (replace \"lowgamma\" with \"gamma\")\n",
    "    merged.columns = [col.replace('lowgamma', 'gamma') for col in merged.columns]\n",
    "\n",
    "    return merged\n",
    "\n",
    "def make_conn_df(conns, conn_labels, subject_id, task):\n",
    "    # initiate an empty df\n",
    "    conn_df = pd.DataFrame()\n",
    "    for freq, conn in conns.items():\n",
    "        conn_flat = conn[np.triu_indices(conn.shape[-1], k=1)]\n",
    "        freq_labels = conn_labels + f' ({freq})'\n",
    "        conn_df_ = pd.DataFrame(conn_flat, index=freq_labels, columns=[0]).transpose()\n",
    "        conn_df = conn_df.join(conn_df_, how='outer')\n",
    "        \n",
    "    conn_df = conn_df.set_axis([f'{subject_id}_{task}'])\n",
    "    \n",
    "    return conn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data and preprocess\n",
    "root = 'data/raw/sub-52/'\n",
    "subject = '52'\n",
    "task = 'experience2'\n",
    "\n",
    "# An example of a single subject\n",
    "subject_id = '214911relaxation.vhdr'\n",
    "subject_path = root + subject_id\n",
    "\n",
    "raw = mne.io.read_raw_brainvision(subject_path, preload=True, eog=('EOG1', 'EOG2'), misc=['ECG'])\n",
    "raw.set_channel_types({'ECG': 'ecg'})\n",
    "\n",
    "tmin = 2\n",
    "tmax = 302\n",
    "raw.crop(tmin=tmin, tmax=tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, rejects = preprocessing(raw, subject, task, 'average', [1, 42], 30, 'global')\n",
    "epochs.save(f'data/clean_data/sub-{subject}_ses-01_task-{task}_proc-clean_epo.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = 512 # we resample the data to 512 Hz\n",
    "frequencies = {'delta': (1, 4),\n",
    "               'theta': (4, 8),\n",
    "               'alpha': (8, 13),\n",
    "               'beta': (13, 30),\n",
    "               'lowgamma': (30, 42)  \n",
    "}\n",
    "# Helper functions\n",
    "def create_raw_from_epochs(epochs):\n",
    "    data = np.hstack(epochs.get_data())\n",
    "    info = mne.create_info(ch_names=epochs.ch_names,\n",
    "                           ch_types='eeg',\n",
    "                           sfreq=epochs.info['sfreq'])\n",
    "\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_channel_types({'ECG': 'ecg', 'EOG1': 'eog', 'EOG2': 'eog'})\n",
    "    return raw\n",
    "\n",
    "def make_forward():\n",
    "    # fsaverage files\n",
    "    fs_dir = Path('data/fsaverage')\n",
    "\n",
    "    # The files live in:\n",
    "    trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "    src = fs_dir / 'bem' / 'fsaverage-ico-4-src.fif' # use icosahedron4 with 6.2 mm source spacing\n",
    "    src = mne.read_source_spaces(src)\n",
    "    bem = fs_dir / 'bem' / 'fsaverage-5120-5120-5120-bem-sol.fif'\n",
    "    \n",
    "    path:Path = Path('data/clean_data')\n",
    "    epochs_fname = path / 'sub-01_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(sfreq)\n",
    "\n",
    "    # create raw object\n",
    "    raw = create_raw_from_epochs(epochs)\n",
    "\n",
    "    # insert channel positions\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    raw.set_montage(montage)\n",
    "\n",
    "    # forward solution (the same across all subjects)\n",
    "    fwd = mne.make_forward_solution(raw.info, trans=trans, src=src,\n",
    "                                    bem=bem, eeg=True, mindist=5.0, n_jobs=-1, verbose=False)\n",
    "    del src, bem\n",
    "    return fwd\n",
    "\n",
    "def make_inverse_4baseline(subject: str,\n",
    "                           fwd: mne.forward.forward.Forward,\n",
    "                           path = Path('data/clean_data')):\n",
    "    \n",
    "    epochs_fname = path / f'{subject}_ses-01_task-baseline1_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(512)\n",
    "    \n",
    "    # create raw object\n",
    "    raw = create_raw_from_epochs(epochs)\n",
    "\n",
    "    # insert channel positions\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    raw.set_eeg_reference('average', projection=True)\n",
    "    \n",
    "    # covariance matrix\n",
    "    cov = mne.compute_raw_covariance(raw, method='auto', cv=5, n_jobs=-1)\n",
    "\n",
    "    # inverse operator\n",
    "    inv = mne.minimum_norm.make_inverse_operator(raw.info, fwd, cov, verbose=False)\n",
    "    \n",
    "    return inv\n",
    "\n",
    "def get_labels(subject: str,\n",
    "                     task: str,\n",
    "                     atlas_labels: list,\n",
    "                     inv: mne.minimum_norm.inverse.InverseOperator,\n",
    "                     path:Path = Path('data/clean_data'),\n",
    "                     ):\n",
    "    # open data\n",
    "    epochs_fname = path / f'{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "    epochs.resample(512)\n",
    "    \n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    epochs.set_montage(montage)\n",
    "    \n",
    "    epochs.set_eeg_reference('average', projection=True)\n",
    "\n",
    "    stc = mne.minimum_norm.apply_inverse_epochs(epochs,\n",
    "                                                inv,\n",
    "                                                method='eLORETA',\n",
    "                                                lambda2=1./9.,\n",
    "                                                verbose=False)\n",
    "    \n",
    "    label_ts = mne.extract_label_time_course(stc,\n",
    "                                             atlas_labels,\n",
    "                                             inv['src'],\n",
    "                                             return_generator=False,\n",
    "                                             verbose=False)\n",
    "    \n",
    "    return label_ts\n",
    "\n",
    "def get_connectivity(label_ts,\n",
    "                     frequencies: dict,\n",
    "                     sfreq: int = sfreq\n",
    "):\n",
    "    def bp_gen(label_ts, fmin, fmax, sfreq):\n",
    "        for ts in label_ts:\n",
    "            yield mne.filter.filter_data(ts, sfreq=sfreq, l_freq=fmin, h_freq=fmax)\n",
    "    \n",
    "    # each segment in labels_ts is 1 second\n",
    "    # To compute the connectivity, we want segments that its lenght is about 30 seconds (or a bit less)\n",
    "    label_continious = np.hstack(np.array(label_ts))\n",
    "    label_ts = np.array_split(label_continious, 10, axis=1)\n",
    "    \n",
    "    conns = {} \n",
    "    for bp in frequencies.keys():\n",
    "        conn_obj = envelope_correlation(bp_gen(label_ts, frequencies[bp][0], frequencies[bp][1], sfreq),\n",
    "                                               orthogonalize='pairwise')\n",
    "        conn = conn_obj.combine()\n",
    "        conn = conn.get_data(output='dense')[..., 0]\n",
    "        conns[bp] = conn\n",
    "        \n",
    "    return conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "output_path_labels = f'data/parcellated_source_yeo7/{subject}_task-{task}_labels.npz'\n",
    "output_path_conn = f'data/connectivities/{subject}_task-{task}_conn-corr_filtered.pkl'\n",
    "\n",
    "# we will use Yeo2011 atlas (7 networks)\n",
    "atlas_labels = mne.read_labels_from_annot('fsaverage',\n",
    "                                          'Yeo2011_7Networks_N1000',\n",
    "                                          subjects_dir='data/')\n",
    "\n",
    "# forward solution (same across all subjects)\n",
    "fwd = make_forward()\n",
    "\n",
    "inv = make_inverse_4baseline('sub-52', fwd)\n",
    "label_ts = get_labels(subject, task, atlas_labels, inv)\n",
    "# np.savez_compressed(output_path_labels, labels=label_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEC\n",
    "#### at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claculate or read connectivity data\n",
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "output_path_conn = f'data/connectivities/{subject}_task-{task}_conn-corr_filtered.pkl'\n",
    "\n",
    "# first check if the connectivity data exists in the path\n",
    "if Path(output_path_conn).exists():\n",
    "    with open(output_path_conn, 'rb') as f:\n",
    "        conns = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    conns = get_connectivity(label_ts, frequencies)\n",
    "    with open(output_path_conn, 'wb') as f:\n",
    "            pickle.dump(conns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_df = make_conn_df(conns, net_conn_labels, subject, task)\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', 'sub-52', 'experience2')\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, conn_df], axis=1)\n",
    "sess_data_sub52.index = [258]\n",
    "\n",
    "merged = merge_with_all('data/classification_datasets/others/correlation_source.csv', sess_data_sub52)\n",
    "\n",
    "# save merged data\n",
    "# merged.to_csv('data/classification_datasets/correlation_source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "epochs_fname = f'data/clean_data/{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "\n",
    "frequencies = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'lowgamma': (30, 42)  \n",
    "}\n",
    "\n",
    "# helper function\n",
    "# Envelope Correlation at sensor level\n",
    "def get_PEC_sensor(epochs,\n",
    "                   frequencies: dict,\n",
    "                   sfreq: float = 1000):\n",
    "\n",
    "    def bp_gen(epochs, fmin, fmax, sfreq):\n",
    "        for epoch in epochs:\n",
    "            yield mne.filter.filter_data(epoch, sfreq=sfreq, l_freq=fmin, h_freq=fmax)\n",
    "    \n",
    "    # each segment in epochs is 1 second\n",
    "    raw = np.hstack(epochs.get_data())\n",
    "    \n",
    "    # To compute the connectivity, we want segments that its length is about 30 seconds (or a bit less)\n",
    "    epochs = np.array_split(raw, 10, axis=1)\n",
    "    \n",
    "    conns = {} \n",
    "    for bp in frequencies.keys():\n",
    "        conn_obj = envelope_correlation(bp_gen(epochs, frequencies[bp][0], frequencies[bp][1], sfreq),\n",
    "                                               orthogonalize='pairwise')\n",
    "        conn = conn_obj.combine()\n",
    "        conn = conn.get_data(output='dense')[..., 0]\n",
    "        conns[bp] = conn\n",
    "        \n",
    "    return conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate or read connectivity data\n",
    "connectivity_path = f'data/connectivities/correlation_sensor/{subject}_task-{task}_conn-corr_sensor_filtered.pkl'\n",
    "if Path(connectivity_path).exists():\n",
    "    with open(connectivity_path, 'rb') as f:\n",
    "        conns = pickle.load(f)\n",
    "else:\n",
    "    print(f'>>>>> Calculating Connectivity Data for {subject}, {task} task <<<<<')\n",
    "    # open epochs\n",
    "    epochs = mne.read_epochs(epochs_fname, preload=True)\n",
    "\n",
    "    # set montage\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    epochs.set_montage(montage)\n",
    "        \n",
    "    # surface laplacian\n",
    "    epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs_csd.drop_channels(ch_names=['M1', 'M2', 'EOG1', 'EOG2', 'ECG'])\n",
    "\n",
    "    # compute connectivity\n",
    "    conns = get_PEC_sensor(epochs_csd, frequencies, sfreq=1000)\n",
    "    \n",
    "    # # save connectivity\n",
    "    # with open(connectivity_path, 'wb') as f:\n",
    "    #     pickle.dump(conns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_df = make_conn_df(conns, sensor_conn_labels, '52', task)\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, conn_df], axis=1)\n",
    "sess_data_sub52.index = [258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_with_all('data/classification_datasets/others/correlation_sensor.csv', sess_data_sub52)\n",
    "# save merged data\n",
    "merged.to_csv('data/classification_datasets/correlation_sensor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wPLI\n",
    "#### at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "freqs = np.arange(1, 42, 1)\n",
    "foi = np.array([\n",
    "        [1, 4],\n",
    "        [4, 8],\n",
    "        [8, 13],\n",
    "        [13, 30],\n",
    "        [30, 42]\n",
    "        ])\n",
    "fmin = tuple(foi[:, 0])\n",
    "fmax = tuple(foi[:, 1])\n",
    "\n",
    "## bands\n",
    "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "# define a function that get a numpy array and return a dataframe of FCs\n",
    "def calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, conn_labels, sfreq,\n",
    "                   verbose=0, decim=20):\n",
    "  conn = spectral_connectivity_time(data,\n",
    "                                    freqs=freqs,\n",
    "                                    method='wpli',\n",
    "                                    fmin=fmin,\n",
    "                                    fmax=fmax,\n",
    "                                    sfreq=sfreq,\n",
    "                                    faverage=True,\n",
    "                                    average=True,\n",
    "                                    verbose=verbose,\n",
    "                                    decim=decim,\n",
    "                                    n_cycles=5\n",
    "                                  )\n",
    "\n",
    "  # get data\n",
    "  conn = conn.get_data(output='dense')\n",
    "  \n",
    "  # create a dataframe from a dictionary of connectivities in different bands\n",
    "  temp = {}\n",
    "  for i, b in enumerate(bands):\n",
    "      temp[b] = conn[:, :, i][np.tril_indices(conn.shape[0], k=-1)]\n",
    "\n",
    "  df = pd.DataFrame(temp, index=conn_labels).stack().reset_index()\n",
    "  df['conn'] = df['level_0'] + '_' + df['level_1']\n",
    "  \n",
    "  return df.drop(columns=['level_0', 'level_1']).set_index('conn').T.set_index([pd.Index([subject + '_' + task])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-52'\n",
    "task = 'experience2'\n",
    "subject_path = f'data/parcellated_source_yeo7/{subject}_task-{task}_labels.npz'\n",
    "label_ts = np.load(subject_path)['labels']\n",
    "data = np.hstack(label_ts)\n",
    "\n",
    "# To get 30-seconds time segments, we need to split source arrays into 10 segments.\n",
    "# To make the array divisible by 10, we cut off a few data points (less than 10) at the end:\n",
    "idx = data.shape[1] - data.shape[1]%10\n",
    "data = data[:, :idx]\n",
    "data = np.array(np.split(data, 10, axis=1))\n",
    "\n",
    "# calculate wpli\n",
    "df = calculate_wpli(data, freqs, fmin, fmax, bands, subject=subject, task=task, conn_labels=net_conn_labels,\n",
    "                    verbose=0, decim=10, sfreq=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session data for a single subject\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "df.index = ['52_experience2'] # rename connectivity index to make it match with session data index\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, df], axis=1)\n",
    "# # merge subject data with all data\n",
    "merged = merge_with_all('data/classification_datasets/others/wpli_source.csv', sess_data_sub52)\n",
    "# merged.to_csv('data/classification_datasets/wpli_source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_fname = f'data/clean_data/{subject}_ses-01_task-{task}_proc-clean_epo.fif'\n",
    "\n",
    "epochs = mne.read_epochs(epochs_fname, verbose=0)\n",
    "epochs.drop_channels(['M1', 'M2'])\n",
    "epochs.pick_types(eeg=True)\n",
    "\n",
    "# set montage\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "epochs.set_montage(montage)\n",
    "print(epochs.info)\n",
    "\n",
    "# surface laplacian\n",
    "epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "data = np.array(np.split(np.hstack(epochs_csd.get_data()), 10, axis=1))\n",
    "\n",
    "df = calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, sensor_conn_labels, sfreq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session data for a single subject\n",
    "sess_data_sub52 = get_session_data_for_sub('data/behavioral_data/PLB_HYP_data_MASTER.xlsx', int(52), 'experience', '2')\n",
    "df.index = ['52_experience2'] # rename connectivity index to make it match with session data index\n",
    "sess_data_sub52 = pd.concat([sess_data_sub52, df], axis=1)\n",
    "# # merge subject data with all data\n",
    "merged = merge_with_all('data/classification_datasets/others/wpli_sensor.csv', sess_data_sub52)\n",
    "merged.to_csv('data/classification_datasets/wpli_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/PLB_HYP_data_MASTER.xlsx')\n",
    "id_vars=['timestamp_trigger_4_recording_1',\n",
    " 'timestamp_trigger_4_recording_2',\n",
    " 'timestamp_trigger_4_recording_3',\n",
    " 'timestamp_trigger_4_recording_4',\n",
    " 'timestamp_trigger_5_recording_1',\n",
    " 'timestamp_trigger_5_recording_2',\n",
    " 'timestamp_trigger_5_recording_3',\n",
    " 'timestamp_trigger_5_recording_4',\n",
    " 'timestamp_trigger_6_recording_1',\n",
    " 'timestamp_trigger_6_recording_2',\n",
    " 'timestamp_trigger_6_recording_3',\n",
    " 'timestamp_trigger_6_recording_4',\n",
    " 'timestamp_trigger_7_recording_1',\n",
    " 'timestamp_trigger_7_recording_2',\n",
    " 'timestamp_trigger_7_recording_3',\n",
    " 'timestamp_trigger_7_recording_4',\n",
    " ]\n",
    "\n",
    "df = df.melt(id_vars=id_vars,\n",
    " value_vars=['procedure_type_1',\n",
    " 'procedure_type_2',\n",
    " 'procedure_type_3',\n",
    " 'procedure_type_4'])\n",
    "\n",
    "df['session'] = df['variable'].apply(lambda x: x.split('_')[-1])\n",
    "df['timestamp'] = df.apply(lambda r: r['timestamp_trigger_4_recording_'+r['session']], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SugNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
