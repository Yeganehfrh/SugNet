{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wPLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import mne\n",
    "from mne_connectivity import spectral_connectivity_time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATLAB\n",
    "## Prepare raw data for MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6',\n",
    "             'P1', 'P2', 'P3', 'P4', 'P5', 'P6']\n",
    "\n",
    "for file_dir in sorted(Path('data/clean_data/').glob('*.fif')):\n",
    "    subject, task = re.search('(.*)_ses-01_task-(.*)_proc-clean_epo', file_dir.stem).groups()\n",
    "    \n",
    "    # open data\n",
    "    epochs = mne.read_epochs(file_dir, verbose=0)\n",
    "    epochs.pick_channels(target_ch)\n",
    "    \n",
    "    # epoch to continuous\n",
    "    ch_names = epochs.ch_names\n",
    "    data = np.hstack(epochs.get_data())\n",
    "    info = mne.create_info(ch_names,\n",
    "                       1000,\n",
    "                       len(ch_names)*['eeg'],\n",
    "                       )\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.save(f'data/Data4MATLAB_PLI/{subject}_{task}_raw.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open mat file and create datatable for wPLI connectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wpli = loadmat('all_wpli.mat')\n",
    "\n",
    "# create a list of files\n",
    "trial_list = []\n",
    "for file_dir in sorted(Path('data/clean_data/').glob('*.fif')):\n",
    "    subject, task = re.search('(.*)_ses-01_task-(.*)_proc-clean_epo', file_dir.stem).groups()\n",
    "    trial_list.append(subject + '_' + task)\n",
    "\n",
    "wpli_agg = {}\n",
    "for i in range(len(trial_list)):\n",
    "    wpli_agg[trial_list[i]] = all_wpli['all_conns'][0][0][i].mean(1)\n",
    "    \n",
    "# connectivity labels\n",
    "conn_info = all_wpli['all_conns'][0][0][-1][0][0][0]\n",
    "\n",
    "conn_labels = []\n",
    "for i in range(len(conn_info)):\n",
    "    conn_labels.append(conn_info[i][0][0] + '_' + conn_info[i][1][0])\n",
    "\n",
    "conn_df = pd.DataFrame.from_dict(wpli_agg, columns=conn_labels, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for frontoparietal connections\n",
    "inter_conn = [i\n",
    "             for i in conn_labels\n",
    "             if i[0] + i[3] == 'PP' or i[0] + i[3] == 'FF']\n",
    "ind = [np.where(np.array(conn_labels) == i)[0][0] for i in inter_conn]\n",
    "frontal_parietal = np.delete(conn_labels, ind)\n",
    "\n",
    "frontal_parietal = {\n",
    "'left-frontal_left-pariental' : ['P3_F3', 'P1_F3', 'P5_F3', 'F1_P3', 'F5_P3', 'P1_F1', 'P5_F1', 'P1_F5', 'P5_F5'],\n",
    "'left-frontal_right-pariental': ['P4_F3', 'P6_F3', 'P2_F3', 'F1_P4', 'F5_P4', 'P2_F1', 'P6_F1', 'P2_F5', 'P6_F5'],\n",
    "'right-frontal_left-pariental' : ['P3_F4', 'P5_F4', 'F2_P3', 'P1_F2', 'P5_F2', 'P1_F6', 'P5_F6', 'P1_F4', 'F6_P3'],\n",
    "'right-frontal_right-pariental' : ['P4_F4', 'P2_F4', 'P6_F4', 'F2_P4', 'F6_P4', 'P2_F2', 'P6_F2', 'P2_F6', 'P6_F6']\n",
    "}\n",
    "\n",
    "# aggregate connections between frontal and parietal areas\n",
    "fp_df = pd.DataFrame()\n",
    "for k,v in frontal_parietal.items():\n",
    "    fp_df[k] = conn_df[v].mean(1)\n",
    "\n",
    "# create labels for connections between traget channels\n",
    "target_ch = ['P4', 'P3', 'F4', 'F3']\n",
    "target_connections = pd.DataFrame(columns=target_ch, index=target_ch)\n",
    "target_connections = target_connections.apply(lambda x: x.index + '_' + x.name)\n",
    "target_connections = target_connections.values[np.triu_indices(target_connections.shape[0], k=1)]\n",
    "\n",
    "# join aggregated connectivities and target channel connectivities\n",
    "df = fp_df.join(conn_df[target_connections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with behavioral data\n",
    "# open behavioral data and ids map\n",
    "bh = pd.read_csv('data/behavioral_data/archived/plb_hyp_data.csv', index_col='index')\n",
    "ids_map = pd.read_excel('docs/ids_map.xlsx', header=1, index_col='behavioral_id')\n",
    "ids_map = ids_map.drop_duplicates('bids_id')\n",
    "ids_map = ids_map[['bids_id']]\n",
    "ids_map['bids_id'] = ids_map['bids_id'].apply(lambda x:str(x).zfill(2))\n",
    "bh = bh.join(ids_map, how='right')\n",
    "bh = bh.melt(\n",
    "    id_vars=['procedure_type_1', 'procedure_type_2', 'procedure_type_3', 'procedure_type_4', 'bids_id',\n",
    "             'description_type_1', 'description_type_2', 'description_type_3', 'description_type_4'],\n",
    "    value_vars=['hypnosis_depth_1', 'hypnosis_depth_2', 'hypnosis_depth_3', 'hypnosis_depth_4'])\n",
    "bh['session'] = bh['variable'].apply(lambda x:x.split('_')[2])\n",
    "bh['procedure'] = bh.apply(lambda r: r['procedure_type_'+r['session']], axis=1)\n",
    "bh['description'] = bh.apply(lambda r: r['description_type_'+r['session']], axis=1)\n",
    "bh = bh[['bids_id', 'value', 'procedure', 'description', 'session']].sort_values(by=['bids_id', 'session']).set_index('bids_id')\n",
    "bh = bh.rename(columns={'value':'hypnosis_depth'})\n",
    "bh.reset_index(inplace=True)\n",
    "\n",
    "df[['bids_id', 'condition']] = df.index.to_series().apply(lambda x:x.split('_')).apply(pd.Series)\n",
    "df['session'] = df['condition'].apply(lambda x:x[-1])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['bids_id'] = df['bids_id'].apply(lambda x:x[4:])\n",
    "df = pd.merge(bh, df, how='right', on=['session', 'bids_id'], right_index=False)\n",
    "df = df.sort_values(by=['bids_id', 'session', 'condition']).reset_index(drop=True)\n",
    "df.insert(1, 'condition', df.pop('condition'))\n",
    "df.head()\n",
    "\n",
    "# correct for procedure, description, and hypnosis depth of baseline rows\n",
    "df.loc[df['condition'] == 'baseline1', ['procedure', 'description']] = 'baseline'\n",
    "df.loc[df['condition'] == 'baseline2', ['procedure', 'description']] = 'baseline'\n",
    "df.loc[df['condition'] == 'baseline1', ['hypnosis_depth']] = None\n",
    "df.loc[df['condition'] == 'baseline2', ['hypnosis_depth']] = None\n",
    "\n",
    "# save\n",
    "# df.to_csv('wpli_alpha2.csv', index=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python\n",
    "## wPLI at sensor level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "freqs = np.arange(1, 42, 1)\n",
    "foi = np.array([\n",
    "        [1, 4],\n",
    "        [4, 8],\n",
    "        [8, 13],\n",
    "        [13, 30],\n",
    "        [30, 42]\n",
    "        ])\n",
    "fmin = tuple(foi[:, 0])\n",
    "fmax = tuple(foi[:, 1])\n",
    "\n",
    "## open a sample eeg data and create connectivity labels based on its channel names\n",
    "epochs = mne.read_epochs('data/clean_data/sub-01_ses-01_task-experience1_proc-clean_epo.fif',\n",
    "                         verbose=0)\n",
    "epochs.drop_channels(['M1', 'M2'])\n",
    "epochs.pick_types(eeg=True)\n",
    "ch_names = epochs.ch_names\n",
    "\n",
    "conn_labels = pd.DataFrame(columns=ch_names, index=ch_names)\n",
    "conn_labels = conn_labels.apply(lambda x: x.index + '\\N{left right arrow}' + x.name)\n",
    "conn_labels = conn_labels.values[np.tril_indices(conn_labels.shape[0], k=-1)]\n",
    "\n",
    "## bands\n",
    "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "# define a function that get a numpy array and return a dataframe of FCs\n",
    "def calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, conn_labels, sfreq,\n",
    "                   verbose=0, decim=20):\n",
    "  conn = spectral_connectivity_time(data,\n",
    "                                    freqs=freqs,\n",
    "                                    method='wpli',\n",
    "                                    fmin=fmin,\n",
    "                                    fmax=fmax,\n",
    "                                    sfreq=sfreq,\n",
    "                                    faverage=True,\n",
    "                                    average=True,\n",
    "                                    verbose=verbose,\n",
    "                                    decim=decim,\n",
    "                                    n_cycles=5\n",
    "                                  )\n",
    "\n",
    "  # get data\n",
    "  conn = conn.get_data(output='dense')\n",
    "  \n",
    "  # create a dataframe from a dictionary of connectivities in different bands\n",
    "  temp = {}\n",
    "  for i, b in enumerate(bands):\n",
    "      temp[b] = conn[:, :, i][np.tril_indices(conn.shape[0], k=-1)]\n",
    "\n",
    "  df = pd.DataFrame(temp, index=conn_labels).stack().reset_index()\n",
    "  df['conn'] = df['level_0'] + '_' + df['level_1']\n",
    "  \n",
    "  return df.drop(columns=['level_0', 'level_1']).set_index('conn').T.set_index([pd.Index([subject + '_' + task])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame([]) # connectivity collectors\n",
    "\n",
    "for subject_path in sorted(Path('data/clean_data').glob('*.fif')):\n",
    "    subject, task = re.search('(.*)_ses-01_task-(.*)_proc-clean_epo.*', subject_path.stem).groups()\n",
    "    \n",
    "    if 'experience' in task:\n",
    "        # open data and modify\n",
    "        print(f'>>>>>>>>>>>>>> Processing {subject} -- {task}')\n",
    "        epochs = mne.read_epochs(subject_path, verbose=0)\n",
    "        epochs.drop_channels(['M1', 'M2'])\n",
    "        epochs.pick_types(eeg=True)\n",
    "        data = np.array(np.split(np.hstack(epochs.get_data()), 10, axis=1))\n",
    "        \n",
    "        df = calculate_wpli(data, freqs, fmin, fmax, bands, subject, task, sfreq=1000)\n",
    "        \n",
    "        df_total = pd.concat([df_total, df])\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavioral data\n",
    "bh = pd.read_csv('data/behavioral_data/behavioral_data.csv')[:208]\n",
    "bh['bids_id'] = bh['bids_id'].astype('int64')\n",
    "bh['session'] = bh['session'].astype('int64')\n",
    "\n",
    "# append connectivity and behavioral data \n",
    "df_total[['bids_id', 'condition']] = df_total.index.to_series().apply(lambda x:x.split('_')).apply(pd.Series)\n",
    "df_total['session'] = df_total['condition'].apply(lambda x:int(x[-1]))\n",
    "df_total['bids_id'] = df_total['bids_id'].apply(lambda x:int(x[4:]))\n",
    "\n",
    "df_total.reset_index(drop=True, inplace=True)\n",
    "df_total = pd.merge(bh, df_total, how='right', on=['session', 'bids_id'], right_index=False)\n",
    "df_total = df_total.sort_values(by=['bids_id', 'session', 'condition']).reset_index(drop=True)\n",
    "df_total.insert(1, 'condition', df_total.pop('condition'))\n",
    "# df_total.to_csv('data/wpli_sensor.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wPLI at source level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connectivity labels\n",
    "# labels\n",
    "yeo7 = {\n",
    "    'N1': 'Visual',\n",
    "    'N2': 'Somatomotor',\n",
    "    'N3': 'DorsalAttention',\n",
    "    'N4': 'VentralAttention',\n",
    "    'N5': 'Limbic',\n",
    "    'N6': 'Frontoparietal',\n",
    "    'N7': 'Default',\n",
    "    'mwall': 'Medial_Wall',\n",
    "}\n",
    "\n",
    "hemisferes = ['lh', 'rh']\n",
    "\n",
    "# labels based on Yeo2011 atlas orders\n",
    "networks = [yeo7[k]+'_'+ hemisferes[i] for k in yeo7.keys() for i in range(len(hemisferes))]\n",
    "\n",
    "conn_labels = pd.DataFrame(index=networks, columns=networks)\n",
    "conn_labels = conn_labels.apply(lambda x: x.index + '\\N{left right arrow}' + x.name)\n",
    "conn_labels = conn_labels.values[np.tril_indices(conn_labels.shape[0], k=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame([]) # connectivity collectors\n",
    "for subject_path in sorted(Path('data/parcellated_source_yeo7').glob('*.npz')):\n",
    "    subject, task = re.search('(.*)_task-(.*)_labels.*', subject_path.stem).groups()\n",
    "    \n",
    "    if 'experience' in task:\n",
    "        \n",
    "        label_ts = np.load(subject_path)['labels']\n",
    "        data = np.hstack(label_ts)\n",
    "        \n",
    "        # To get 30-second time segments, we need to split source arrays into 10 segments.\n",
    "        # To make the array divisible by 10, we cut off a few data points (less than 10) at the end:\n",
    "        idx = data.shape[1] - data.shape[1]%10\n",
    "        data = data[:, :idx]\n",
    "        data = np.array(np.split(data, 10, axis=1))\n",
    "        \n",
    "        print(f'>>>>>>>>>>>>> Processing {subject} --- {task}')\n",
    "        # calculate wpli\n",
    "        df = calculate_wpli(data, freqs, fmin, fmax, bands, subject=subject, task=task, conn_labels=conn_labels,\n",
    "                            verbose=0, decim=10, sfreq=512)\n",
    "        \n",
    "        # append wpli\n",
    "        df_total = pd.concat([df_total, df])\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for classification\n",
    "# behavioral data\n",
    "bh = pd.read_csv('data/behavioral_data/behavioral_data.csv')[:208]\n",
    "bh['bids_id'] = bh['bids_id'].astype('int64')\n",
    "bh['session'] = bh['session'].astype('int64')\n",
    "\n",
    "# append connectivity and behavioral data \n",
    "df_total[['bids_id', 'condition']] = df_total.index.to_series().apply(lambda x:x.split('_')).apply(pd.Series)\n",
    "df_total['session'] = df_total['condition'].apply(lambda x:int(x[-1]))\n",
    "df_total['bids_id'] = df_total['bids_id'].apply(lambda x:int(x[4:]))\n",
    "\n",
    "df_total.reset_index(drop=True, inplace=True)\n",
    "df_total = pd.merge(bh, df_total, how='right', on=['session', 'bids_id'], right_index=False)\n",
    "df_total = df_total.sort_values(by=['bids_id', 'session', 'condition']).reset_index(drop=True)\n",
    "df_total.insert(1, 'condition', df_total.pop('condition'))\n",
    "df_total.head()\n",
    "# df_total.to_csv('data/classification_datasets/wpli_source.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('otka')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5940ee23a8ed7b2e3c21178d81a306e47a8b6a3c2b3d99c2f75b67b005e8c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
