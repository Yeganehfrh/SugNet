{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wPLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare raw data for MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6',\n",
    "             'P1', 'P2', 'P3', 'P4', 'P5', 'P6']\n",
    "\n",
    "for file_dir in sorted(Path('data/clean_data/').glob('*.fif')):\n",
    "    subject, task = re.search('(.*)_ses-01_task-(.*)_proc-clean_epo', file_dir.stem).groups()\n",
    "    \n",
    "    # open data\n",
    "    epochs = mne.read_epochs(file_dir, verbose=0)\n",
    "    epochs.pick_channels(target_ch)\n",
    "    \n",
    "    # epoch to continuous\n",
    "    ch_names = epochs.ch_names\n",
    "    data = np.hstack(epochs.get_data())\n",
    "    info = mne.create_info(ch_names,\n",
    "                       1000,\n",
    "                       len(ch_names)*['eeg'],\n",
    "                       )\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.save(f'data/Data4MATLAB_PLI/{subject}_{task}_raw.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open mat file and create datatable for wPLI connectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wpli = loadmat('all_wpli.mat')\n",
    "\n",
    "# create a list of files\n",
    "trial_list = []\n",
    "for file_dir in sorted(Path('data/clean_data/').glob('*.fif')):\n",
    "    subject, task = re.search('(.*)_ses-01_task-(.*)_proc-clean_epo', file_dir.stem).groups()\n",
    "    trial_list.append(subject + '_' + task)\n",
    "\n",
    "wpli_agg = {}\n",
    "for i in range(len(trial_list)):\n",
    "    wpli_agg[trial_list[i]] = all_wpli['all_conns'][0][0][i].mean(1)\n",
    "    \n",
    "# connectivity labels\n",
    "conn_info = all_wpli['all_conns'][0][0][-1][0][0][0]\n",
    "\n",
    "conn_labels = []\n",
    "for i in range(len(conn_info)):\n",
    "    conn_labels.append(conn_info[i][0][0] + '_' + conn_info[i][1][0])\n",
    "\n",
    "conn_df = pd.DataFrame.from_dict(wpli_agg, columns=conn_labels, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for frontoparietal connections\n",
    "inter_conn = [i\n",
    "             for i in conn_labels\n",
    "             if i[0] + i[3] == 'PP' or i[0] + i[3] == 'FF']\n",
    "ind = [np.where(np.array(conn_labels) == i)[0][0] for i in inter_conn]\n",
    "frontal_parietal = np.delete(conn_labels, ind)\n",
    "\n",
    "frontal_parietal = {\n",
    "'left-frontal_left-pariental' : ['P3_F3', 'P1_F3', 'P5_F3', 'F1_P3', 'F5_P3', 'P1_F1', 'P5_F1', 'P1_F5', 'P5_F5'],\n",
    "'left-frontal_right-pariental': ['P4_F3', 'P6_F3', 'P2_F3', 'F1_P4', 'F5_P4', 'P2_F1', 'P6_F1', 'P2_F5', 'P6_F5'],\n",
    "'right-frontal_left-pariental' : ['P3_F4', 'P5_F4', 'F2_P3', 'P1_F2', 'P5_F2', 'P1_F6', 'P5_F6', 'P1_F4', 'F6_P3'],\n",
    "'right-frontal_right-pariental' : ['P4_F4', 'P2_F4', 'P6_F4', 'F2_P4', 'F6_P4', 'P2_F2', 'P6_F2', 'P2_F6', 'P6_F6']\n",
    "}\n",
    "\n",
    "# aggregate connections between frontal and parietal areas\n",
    "fp_df = pd.DataFrame()\n",
    "for k,v in frontal_parietal.items():\n",
    "    fp_df[k] = conn_df[v].mean(1)\n",
    "\n",
    "# create labels for connections between traget channels\n",
    "target_ch = ['P4', 'P3', 'F4', 'F3']\n",
    "target_connections = pd.DataFrame(columns=target_ch, index=target_ch)\n",
    "target_connections = target_connections.apply(lambda x: x.index + '_' + x.name)\n",
    "target_connections = target_connections.values[np.triu_indices(target_connections.shape[0], k=1)]\n",
    "\n",
    "# join aggregated connectivities and target channel connectivities\n",
    "df = fp_df.join(conn_df[target_connections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with behavioral data\n",
    "# open behavioral data and ids map\n",
    "bh = pd.read_csv('data/behavioral_data/archived/plb_hyp_data.csv', index_col='index')\n",
    "ids_map = pd.read_excel('docs/ids_map.xlsx', header=1, index_col='behavioral_id')\n",
    "ids_map = ids_map.drop_duplicates('bids_id')\n",
    "ids_map = ids_map[['bids_id']]\n",
    "ids_map['bids_id'] = ids_map['bids_id'].apply(lambda x:str(x).zfill(2))\n",
    "bh = bh.join(ids_map, how='right')\n",
    "bh = bh.melt(\n",
    "    id_vars=['procedure_type_1', 'procedure_type_2', 'procedure_type_3', 'procedure_type_4', 'bids_id',\n",
    "             'description_type_1', 'description_type_2', 'description_type_3', 'description_type_4'],\n",
    "    value_vars=['hypnosis_depth_1', 'hypnosis_depth_2', 'hypnosis_depth_3', 'hypnosis_depth_4'])\n",
    "bh['session'] = bh['variable'].apply(lambda x:x.split('_')[2])\n",
    "bh['procedure'] = bh.apply(lambda r: r['procedure_type_'+r['session']], axis=1)\n",
    "bh['description'] = bh.apply(lambda r: r['description_type_'+r['session']], axis=1)\n",
    "bh = bh[['bids_id', 'value', 'procedure', 'description', 'session']].sort_values(by=['bids_id', 'session']).set_index('bids_id')\n",
    "bh = bh.rename(columns={'value':'hypnosis_depth'})\n",
    "bh.reset_index(inplace=True)\n",
    "\n",
    "df[['bids_id', 'condition']] = df.index.to_series().apply(lambda x:x.split('_')).apply(pd.Series)\n",
    "df['session'] = df['condition'].apply(lambda x:x[-1])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['bids_id'] = df['bids_id'].apply(lambda x:x[4:])\n",
    "df = pd.merge(bh, df, how='right', on=['session', 'bids_id'], right_index=False)\n",
    "df = df.sort_values(by=['bids_id', 'session', 'condition']).reset_index(drop=True)\n",
    "df.insert(1, 'condition', df.pop('condition'))\n",
    "df.head()\n",
    "\n",
    "# correct for procedure, description, and hypnosis depth of baseline rows\n",
    "df.loc[df['condition'] == 'baseline1', ['procedure', 'description']] = 'baseline'\n",
    "df.loc[df['condition'] == 'baseline2', ['procedure', 'description']] = 'baseline'\n",
    "df.loc[df['condition'] == 'baseline1', ['hypnosis_depth']] = None\n",
    "df.loc[df['condition'] == 'baseline2', ['hypnosis_depth']] = None\n",
    "\n",
    "# save\n",
    "# df.to_csv('wpli_alpha2.csv', index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('otka')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5940ee23a8ed7b2e3c21178d81a306e47a8b6a3c2b3d99c2f75b67b005e8c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
